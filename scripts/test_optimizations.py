"""
Test script to validate all implemented optimizations.

Run this script to verify:
1. Caching system (embedding cache, generation cache)
2. Performance monitoring
3. Dynamic council system
4. Parallel processing
5. Real-time evaluation
6. Audio enhancement utilities
"""

from __future__ import annotations

import json
import tempfile
from pathlib import Path

import numpy as np

# Optional imports
try:
    import soundfile as sf
    HAS_SOUNDFILE = True
except ImportError:
    HAS_SOUNDFILE = False

try:
    from src.embeddings.aligned_embeddings import AlignedEmbedder
    HAS_EMBEDDINGS = True
except ImportError:
    HAS_EMBEDDINGS = False

try:
    from src.evaluation.realtime_evaluator import RealtimeEvaluator
    HAS_REALTIME = True
except ImportError:
    HAS_REALTIME = False

try:
    from src.generators.audio.enhancement import AudioEnhancer
    HAS_AUDIO_ENH = True
except ImportError:
    HAS_AUDIO_ENH = False

try:
    from src.planner.dynamic_council import DynamicSemanticCouncil, PromptAnalyzer
    from src.planner.unified_planner import UnifiedPlannerLLM
    HAS_PLANNER = True
except ImportError:
    HAS_PLANNER = False

from src.utils.cache import EmbeddingCache, GenerationCache, ContentBasedCache
from src.utils.performance_monitor import PerformanceMonitor, measure_performance
from src.utils.parallel_processing import ParallelProcessor


def test_embedding_cache():
    """Test embedding cache functionality."""
    print("\n" + "=" * 60)
    print("TEST 1: Embedding Cache")
    print("=" * 60)
    
    with tempfile.TemporaryDirectory() as tmpdir:
        cache = EmbeddingCache(cache_dir=tmpdir)
        
        # Create test embedding
        test_embedding = np.random.rand(512).astype(np.float32)
        
        # Test set/get
        cache.set("test_text", "text", test_embedding)
        retrieved = cache.get("test_text", "text")
        
        assert retrieved is not None, "Cache should return embedding"
        assert np.allclose(test_embedding, retrieved), "Cached embedding should match original"
        
        # Test cache miss
        miss = cache.get("nonexistent", "text")
        assert miss is None, "Cache should return None for miss"
        
        print("‚úÖ Embedding cache: PASSED")
        return True


def test_generation_cache():
    """Test generation cache functionality."""
    print("\n" + "=" * 60)
    print("TEST 2: Generation Cache")
    print("=" * 60)
    
    if not HAS_SOUNDFILE:
        print("‚ö†Ô∏è  Generation cache test skipped (soundfile not available)")
        return True
    
    with tempfile.TemporaryDirectory() as tmpdir:
        cache = GenerationCache(cache_dir=tmpdir)
        
        # Create test file
        test_file = Path(tmpdir) / "test_output.wav"
        test_audio = np.random.rand(1000).astype(np.float32)
        sf.write(str(test_file), test_audio, 48000)
        
        # Test set/get
        key = cache.set(
            prompt="test prompt",
            modality="audio",
            file_path=str(test_file),
            metadata={"test": "metadata"},
        )
        
        retrieved = cache.get("test prompt", "audio")
        
        assert retrieved is not None, "Cache should return result"
        assert retrieved["modality"] == "audio", "Should return correct modality"
        assert Path(retrieved["file_path"]).exists(), "Cached file should exist"
        
        print("‚úÖ Generation cache: PASSED")
        return True


def test_performance_monitor():
    """Test performance monitoring."""
    print("\n" + "=" * 60)
    print("TEST 3: Performance Monitoring")
    print("=" * 60)
    
    monitor = PerformanceMonitor()
    
    # Test recording metrics
    import time
    
    start = time.time()
    time.sleep(0.1)  # Simulate work
    elapsed = time.time() - start
    
    monitor.record(
        inference_time=elapsed,
        memory_used_mb=100.0,
        batch_size=1,
        device="cpu",
        operation_name="test_operation",
    )
    
    stats = monitor.get_stats("test_operation")
    assert "test_operation" in stats, "Stats should contain operation"
    assert stats["test_operation"].total_calls == 1, "Should have 1 call"
    assert stats["test_operation"].avg_time > 0, "Should have positive time"
    
    # Test context manager
    with measure_performance(monitor, "context_test", batch_size=2):
        time.sleep(0.05)
    
    context_stats = monitor.get_stats("context_test")
    assert context_stats["context_test"].total_calls == 1, "Context manager should record"
    assert context_stats["context_test"].avg_throughput > 0, "Should calculate throughput"
    
    summary = monitor.get_summary()
    assert "test_operation" in summary, "Summary should contain operations"
    assert "context_test" in summary, "Summary should contain context test"
    
    print("‚úÖ Performance monitoring: PASSED")
    return True


def test_cached_embedder():
    """Test cached embedder integration."""
    print("\n" + "=" * 60)
    print("TEST 4: Cached Embedder Integration")
    print("=" * 60)
    
    if not HAS_EMBEDDINGS:
        print("‚ö†Ô∏è  Cached embedder test skipped (embedding models not available)")
        return True
    
    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            # Test with cache enabled
            embedder_cached = AlignedEmbedder(enable_cache=True, cache_dir=tmpdir)
            
            test_text = "A beautiful sunset over the ocean"
            
            # First call - should compute
            emb1 = embedder_cached.embed_text(test_text)
            assert emb1 is not None, "Should return embedding"
            assert len(emb1) == 512, "Should be 512-dimensional"
            
            # Second call - should use cache
            emb2 = embedder_cached.embed_text(test_text)
            assert np.allclose(emb1, emb2), "Cached embedding should match"
            
            # Test with cache disabled
            embedder_no_cache = AlignedEmbedder(enable_cache=False)
            emb3 = embedder_no_cache.embed_text(test_text)
            assert emb3 is not None, "Should work without cache"
            
            print("‚úÖ Cached embedder integration: PASSED")
            return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Cached embedder test skipped (model loading): {e}")
        return True  # Skip if models aren't available


def test_dynamic_council():
    """Test dynamic council system."""
    print("\n" + "=" * 60)
    print("TEST 5: Dynamic Council System")
    print("=" * 60)
    
    if not HAS_PLANNER:
        print("‚ö†Ô∏è  Dynamic council test skipped (planner modules not available)")
        return True
    
    try:
        # Test prompt analyzer
        analyzer = PromptAnalyzer()
        
        # Test visual-dominant prompt
        priority_visual = analyzer.analyze("A beautiful sunset with vibrant colors")
        assert priority_visual.primary in ["text", "image", "audio"], "Should have valid primary"
        assert priority_visual.weights.image_weight >= 0, "Should have weights"
        assert priority_visual.weights.text_weight >= 0, "Should have weights"
        assert priority_visual.weights.audio_weight >= 0, "Should have weights"
        
        # Test audio-dominant prompt
        priority_audio = analyzer.analyze("The sound of rain and distant thunder")
        assert priority_audio.primary in ["text", "image", "audio"], "Should have valid primary"
        
        # Test normalized weights
        normalized = priority_visual.weights.normalize()
        total = normalized.total
        assert abs(total - 3.0) < 0.01, f"Weights should normalize to ~3.0, got {total}"
        
        print("‚úÖ Dynamic council (analyzer): PASSED")
        
        # Test dynamic council (requires LLM, may fail)
        try:
            planner_a = UnifiedPlannerLLM()
            planner_b = UnifiedPlannerLLM()
            planner_c = UnifiedPlannerLLM()
            
            dynamic_council = DynamicSemanticCouncil(
                planner_a=planner_a,
                planner_b=planner_b,
                planner_c=planner_c,
                enable_dynamic_weighting=True,
            )
            
            # Test priority extraction
            priority = dynamic_council.get_modality_priority("A quiet forest")
            assert priority is not None, "Should return priority"
            
            print("‚úÖ Dynamic council (full): PASSED")
        except Exception as e:
            print(f"‚ö†Ô∏è  Dynamic council (full) skipped (LLM unavailable): {e}")
        
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Dynamic council test skipped: {e}")
        return True


def _square_func(x: int) -> int:
    """Top-level function for pickling."""
    return x * x


def _sum_batch_func(batch: list[int]) -> list[int]:
    """Top-level function for pickling."""
    return [sum(batch)] * len(batch)


def test_parallel_processing():
    """Test parallel processing utilities."""
    print("\n" + "=" * 60)
    print("TEST 6: Parallel Processing")
    print("=" * 60)
    
    # Test simple parallel map with threads (avoids pickling issues)
    processor = ParallelProcessor(max_workers=2, use_threads=True)
    items = list(range(10))
    results = processor.map(_square_func, items)
    
    assert len(results) == len(items), "Should process all items"
    # Results may not preserve order with threads, so check values match
    assert sorted(results) == sorted([x * x for x in items]), "Results should match sequential"
    
    # Test batch processing with threads
    batch_results = processor.batch_map(_sum_batch_func, items, batch_size=3)
    assert len(batch_results) == len(items), "Should process all items"
    
    print("‚úÖ Parallel processing: PASSED")
    return True


def test_realtime_evaluator():
    """Test real-time evaluator."""
    print("\n" + "=" * 60)
    print("TEST 7: Real-Time Evaluator")
    print("=" * 60)
    
    if not HAS_REALTIME:
        print("‚ö†Ô∏è  Real-time evaluator test skipped (module not available)")
        return True
    
    with tempfile.TemporaryDirectory() as tmpdir:
        # Mock evaluation function
        def mock_evaluate(sample):
            return {
                "scores": {
                    "msci": 0.5,
                    "st_i": 0.6,
                    "st_a": 0.4,
                    "si_a": 0.55,
                },
                "coherence": {"classification": {"label": "HIGH_COHERENCE"}},
                "metadata": {"sample_id": sample.get("id", "unknown")},
            }
        
        evaluator = RealtimeEvaluator(
            evaluation_func=mock_evaluate,
            output_dir=tmpdir,
            enable_monitoring=False,  # Disable to avoid model loading
        )
        
        # Test stream evaluation
        samples = [{"id": f"sample_{i}"} for i in range(5)]
        results = list(evaluator.evaluate_stream(iter(samples)))
        
        assert len(results) == 5, "Should evaluate all samples"
        assert all(r.scores.get("msci") == 0.5 for r in results), "Should have correct scores"
        
        # Test batch evaluation
        evaluator.reset()
        batch_results = evaluator.evaluate_batch(samples)
        assert len(batch_results) == 5, "Should evaluate batch"
        
        # Test aggregate stats
        stats = evaluator.get_aggregate_stats()
        assert stats["total_samples"] == 5, "Should have 5 samples"
        assert "msci" in stats, "Should have MSCI stats"
        
        print("‚úÖ Real-time evaluator: PASSED")
        return True


def test_audio_enhancement():
    """Test audio enhancement utilities."""
    print("\n" + "=" * 60)
    print("TEST 8: Audio Enhancement")
    print("=" * 60)
    
    if not HAS_AUDIO_ENH or not HAS_SOUNDFILE:
        print("‚ö†Ô∏è  Audio enhancement test skipped (dependencies not available)")
        return True
    
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create test audio file
        test_audio_path = Path(tmpdir) / "test_audio.wav"
        duration = 2.0
        sr = 48000
        t = np.linspace(0, duration, int(sr * duration), endpoint=False)
        audio_data = np.sin(2 * np.pi * 440 * t).astype(np.float32)  # 440 Hz tone
        sf.write(str(test_audio_path), audio_data, sr)
        
        enhancer = AudioEnhancer(sample_rate=sr)
        
        # Test segmentation
        segments = enhancer.segment_audio(str(test_audio_path), segment_duration=0.5)
        assert len(segments) > 0, "Should create segments"
        assert all(seg.sample_rate == sr for seg in segments), "Segments should have correct SR"
        
        # Test analysis
        analysis = enhancer.analyze_audio(str(test_audio_path))
        assert analysis.duration > 0, "Should have duration"
        assert analysis.sample_rate == sr, "Should have correct SR"
        assert len(analysis.segments) > 0, "Should have segments"
        
        # Test background/foreground separation
        bg, fg = enhancer.separate_background_foreground(str(test_audio_path))
        assert len(bg) == len(fg), "Should have same length"
        assert len(bg) == len(audio_data), "Should match input length"
        
        # Test temporal coherence improvement
        enhanced = enhancer.improve_temporal_coherence(str(test_audio_path))
        assert len(enhanced) > 0, "Should return enhanced audio"
        
        print("‚úÖ Audio enhancement: PASSED")
        return True


def main():
    """Run all optimization tests."""
    print("\n" + "=" * 60)
    print("OPTIMIZATION TEST SUITE")
    print("=" * 60)
    
    tests = [
        ("Embedding Cache", test_embedding_cache),
        ("Generation Cache", test_generation_cache),
        ("Performance Monitor", test_performance_monitor),
        ("Cached Embedder", test_cached_embedder),
        ("Dynamic Council", test_dynamic_council),
        ("Parallel Processing", test_parallel_processing),
        ("Real-Time Evaluator", test_realtime_evaluator),
        ("Audio Enhancement", test_audio_enhancement),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            passed = test_func()
            results.append((test_name, passed, None))
        except Exception as e:
            print(f"‚ùå {test_name}: FAILED - {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False, str(e)))
    
    # Summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for _, p, _ in results if p)
    total = len(results)
    
    for test_name, passed, error in results:
        status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
        print(f"{status}: {test_name}")
        if error:
            print(f"   Error: {error}")
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ All tests passed!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed")
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
