{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Learned Projection Heads for Embedding Alignment\n",
    "\n",
    "This notebook trains projection heads to align CLIP, CLAP, and text embeddings in a shared semantic space.\n",
    "\n",
    "**Purpose**: Improve cross-modal similarity by learning to map different embedding spaces together.\n",
    "\n",
    "**Requirements**:\n",
    "- GPU runtime (T4 or better)\n",
    "- ~2-4 hours training time\n",
    "- Training data: aligned (text, image, audio) triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip install torch sentence-transformers transformers laion-clap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if running on Colab)\n",
    "# !git clone https://github.com/your-repo/MultiModal-Coherence-AI.git\n",
    "# %cd MultiModal-Coherence-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Training Data\n",
    "\n",
    "We need aligned (text, image, audio) triplets. Options:\n",
    "1. Use generated experiment data from runs/\n",
    "2. Use AudioCaps + LAION with synthetic pairing\n",
    "3. Create custom aligned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from experiment runs\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "def load_triplets_from_runs(runs_dir: str, max_samples: int = 5000):\n",
    "    \"\"\"Load triplets from experiment bundles.\"\"\"\n",
    "    bundles = glob(f\"{runs_dir}/**/bundle.json\", recursive=True)\n",
    "    \n",
    "    texts, images, audios = [], [], []\n",
    "    \n",
    "    for bundle_path in bundles[:max_samples]:\n",
    "        try:\n",
    "            with open(bundle_path) as f:\n",
    "                bundle = json.load(f)\n",
    "            \n",
    "            text = bundle.get('outputs', {}).get('text', '')\n",
    "            img_path = Path(bundle_path).parent / 'image' / 'output.png'\n",
    "            aud_path = Path(bundle_path).parent / 'audio' / 'output.wav'\n",
    "            \n",
    "            if text and img_path.exists() and aud_path.exists():\n",
    "                texts.append(text)\n",
    "                images.append(str(img_path))\n",
    "                audios.append(str(aud_path))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} triplets from runs\")\n",
    "    return texts, images, audios\n",
    "\n",
    "# Uncomment to use:\n",
    "# texts, images, audios = load_triplets_from_runs('../runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Create synthetic triplets (demonstration)\n",
    "# In practice, you'd load real aligned data\n",
    "\n",
    "def create_synthetic_demo_data(n_samples: int = 1000):\n",
    "    \"\"\"Create random embeddings for demonstration.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate embeddings with some alignment\n",
    "    base = np.random.randn(n_samples, 256)\n",
    "    \n",
    "    text_emb = base + np.random.randn(n_samples, 256) * 0.3\n",
    "    image_emb = base + np.random.randn(n_samples, 256) * 0.3\n",
    "    audio_emb = base + np.random.randn(n_samples, 256) * 0.3\n",
    "    \n",
    "    # Pad to 512 dim (typical embedding size)\n",
    "    text_emb = np.pad(text_emb, ((0, 0), (0, 256)))\n",
    "    image_emb = np.pad(image_emb, ((0, 0), (0, 256)))\n",
    "    audio_emb = np.pad(audio_emb, ((0, 0), (0, 256)))\n",
    "    \n",
    "    return text_emb, image_emb, audio_emb\n",
    "\n",
    "# For demonstration\n",
    "text_emb, image_emb, audio_emb = create_synthetic_demo_data(2000)\n",
    "print(f\"Shapes: text={text_emb.shape}, image={image_emb.shape}, audio={audio_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.learned_projection import LearnedProjection\n",
    "from src.training.contrastive_trainer import (\n",
    "    MultimodalTripletDataset,\n",
    "    ContrastiveTrainer,\n",
    "    TrainingConfig,\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "n_train = int(len(text_emb) * 0.9)\n",
    "\n",
    "train_dataset = MultimodalTripletDataset(\n",
    "    text_embeddings=text_emb[:n_train],\n",
    "    image_embeddings=image_emb[:n_train],\n",
    "    audio_embeddings=audio_emb[:n_train],\n",
    ")\n",
    "\n",
    "val_dataset = MultimodalTripletDataset(\n",
    "    text_embeddings=text_emb[n_train:],\n",
    "    image_embeddings=image_emb[n_train:],\n",
    "    audio_embeddings=audio_emb[n_train:],\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = LearnedProjection(\n",
    "    text_dim=512,\n",
    "    image_dim=512,\n",
    "    audio_dim=512,\n",
    "    shared_dim=256,\n",
    "    hidden_dim=384,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = TrainingConfig(\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    n_epochs=10,\n",
    "    temperature=0.07,\n",
    "    eval_every=100,\n",
    "    save_every=500,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "print(f\"Training on: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = ContrastiveTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    output_dir=Path(\"../models/projection\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trained_model = trainer.train(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "if trainer.history:\n",
    "    steps = [h['step'] for h in trainer.history]\n",
    "    losses = [h['val_loss'] for h in trainer.history]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(steps, losses)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Training Progress')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test projection quality\n",
    "trained_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get some test samples\n",
    "    test_text = torch.tensor(text_emb[n_train:n_train+100], dtype=torch.float32)\n",
    "    test_image = torch.tensor(image_emb[n_train:n_train+100], dtype=torch.float32)\n",
    "    test_audio = torch.tensor(audio_emb[n_train:n_train+100], dtype=torch.float32)\n",
    "    \n",
    "    # Project\n",
    "    projected = trained_model(test_text, test_image, test_audio)\n",
    "    \n",
    "    # Compute similarities after projection\n",
    "    p_text = projected['text']\n",
    "    p_image = projected['image']\n",
    "    p_audio = projected['audio']\n",
    "    \n",
    "    # Diagonal = positive pairs\n",
    "    ti_sim = torch.sum(p_text * p_image, dim=-1).mean()\n",
    "    ta_sim = torch.sum(p_text * p_audio, dim=-1).mean()\n",
    "    ia_sim = torch.sum(p_image * p_audio, dim=-1).mean()\n",
    "    \n",
    "    print(f\"\\nPost-projection similarities:\")\n",
    "    print(f\"  Text-Image: {ti_sim:.4f}\")\n",
    "    print(f\"  Text-Audio: {ta_sim:.4f}\")\n",
    "    print(f\"  Image-Audio: {ia_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "save_path = Path(\"../models/projection/learned_projection.pt\")\n",
    "trained_model.save(save_path)\n",
    "print(f\"Model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading\n",
    "loaded_model = LearnedProjection.load(save_path)\n",
    "print(f\"Model loaded successfully. Config: {loaded_model.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with MSCI\n",
    "\n",
    "To use the trained projections with MSCI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using ProjectedEmbedder\n",
    "from src.training.learned_projection import ProjectedEmbedder\n",
    "\n",
    "# Load your base embedder\n",
    "# from src.embeddings.aligned_embeddings import AlignedEmbedder\n",
    "# base_embedder = AlignedEmbedder()\n",
    "\n",
    "# Wrap with projection\n",
    "# projected_embedder = ProjectedEmbedder(base_embedder, loaded_model)\n",
    "\n",
    "# Use for MSCI computation\n",
    "# text_emb = projected_embedder.embed_text(\"A peaceful forest\")\n",
    "# image_emb = projected_embedder.embed_image(\"path/to/image.png\")\n",
    "# audio_emb = projected_embedder.embed_audio(\"path/to/audio.wav\")\n",
    "\n",
    "print(\"See src/training/learned_projection.py for ProjectedEmbedder usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Train on real aligned data (AudioCaps, LAION, experiment runs)\n",
    "2. Evaluate improvement in MSCI-human correlation\n",
    "3. Tune hyperparameters (temperature, learning rate, architecture)\n",
    "4. Compare MSCI v1 vs v2 with projected embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
